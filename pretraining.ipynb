{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a008518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import PicabooLMPretainingDataset, configure_adamw_optimizer\n",
    "from tokenizer import Tokenizer\n",
    "from model import PicabooLMParams, PicabooLM\n",
    "from trainer import TrainerParams, Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from dataclasses import asdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712608d0",
   "metadata": {},
   "source": [
    "# Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "813b1700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(894783, [1068, 692, 649, 101, 358, 115, 296, 258, 429, 111])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer.load(\"models/\")\n",
    "eod_token = \"<|endoftext|>\"\n",
    "with open(\"datasets/combined.txt\",\"r\", encoding=\"utf-8\") as fp:\n",
    "    text = fp.read().replace(\"########\",eod_token)\n",
    "tokens = tokenizer.encode(text=text)\n",
    "len(tokens), tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fb3f33",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be72cefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 7.08M\n",
      "{'context_length': 1024, 'vocab_size': 2048, 'num_blocks': 8, 'num_heads': 8, 'd_model': 256, 'head_dim': 32, 'dropout_rate': 0.1, 'device': 'cpu', 'bias': False}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sample completion: he isshffeeeringarldsally ` death rose probably dou never His Heart seemedab lea ranri creatureiallyokeear closed thoseneingld direct’ked wateralekapped neededved gr dang murph endove\\x12 white slowse takeO read movieals Jakany br ple1es fleinken z guard wo onlyart Even faceL used angumm focreenels being storyific front fear u am\\x1f amadia waved control hopalek wra stre short pointed down read'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params = PicabooLMParams(\n",
    "    context_length=1024,\n",
    "    vocab_size=2048,\n",
    "    num_blocks=8,\n",
    "    num_heads=8,\n",
    "    d_model=256,\n",
    "    head_dim=256//8,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "model = PicabooLM(params=model_params)\n",
    "output_tokens = model.generate(torch.tensor([tokenizer.encode(\"he is\")]), max_new_tokens=100)\n",
    "sample_completion = tokenizer.decode(output_tokens.tolist()[0])\n",
    "print(asdict(model_params))\n",
    "f\"sample completion: {sample_completion}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d38d8f8",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1975d26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps: 432\n",
      "warmup: 0 - 43\n",
      "decay: 44 - 388\n",
      "finalizing: 389 - 432\n"
     ]
    }
   ],
   "source": [
    "train_test_split_pct = 0.99\n",
    "train_test_split_idx = int(len(tokens)*train_test_split_pct)\n",
    "microbatch_size = 2\n",
    "gradient_accumulation_steps = 2\n",
    "epochs = 2\n",
    "effective_batch_size = microbatch_size * gradient_accumulation_steps\n",
    "training_dataset = PicabooLMPretainingDataset(dataset=tokens[:train_test_split_idx],context_size=model_params.context_length)\n",
    "validation_dataset = PicabooLMPretainingDataset(dataset=tokens[train_test_split_idx+1:],context_size=model_params.context_length)\n",
    "training_dataloader = DataLoader(dataset=training_dataset, batch_size=microbatch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(dataset=validation_dataset, batch_size=microbatch_size, shuffle=False)\n",
    "total_steps = len(training_dataset) * epochs // effective_batch_size\n",
    "warmup_steps = int(total_steps * 0.1) # 10% warmup\n",
    "max_steps = int(total_steps * 0.9) # 90% cosine decay\n",
    "print(f\"total_steps: {total_steps}\")\n",
    "print(f\"warmup: 0 - {warmup_steps}\")\n",
    "print(f\"decay: {warmup_steps+1} - {max_steps}\")\n",
    "print(f\"finalizing: {max_steps+1} - {total_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08f3582",
   "metadata": {},
   "source": [
    "On terminal, run `mlflow server --host 127.0.0.1 --port 8000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b658882c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/432 | loss: 7.70 | norm: 4.9528e+00 | dt: 17872.16 ms | lr: 2.33e-05 | tokens/s: 229.18\n",
      "Step 2/432 | loss: 7.59 | norm: 4.4285e+00 | dt: 16858.97 ms | lr: 4.65e-05 | tokens/s: 242.96\n",
      "Step 3/432 | loss: 7.47 | norm: 2.5289e+00 | dt: 16529.77 ms | lr: 6.98e-05 | tokens/s: 247.80\n",
      "Step 4/432 | loss: 7.39 | norm: 2.5241e+00 | dt: 15891.32 ms | lr: 9.30e-05 | tokens/s: 257.75\n",
      "Step 5/432 | loss: 7.31 | norm: 1.9744e+00 | dt: 15357.22 ms | lr: 1.16e-04 | tokens/s: 266.71\n",
      "Step 6/432 | loss: 7.25 | norm: 1.5482e+00 | dt: 17102.38 ms | lr: 1.40e-04 | tokens/s: 239.50\n",
      "Step 7/432 | loss: 7.20 | norm: 1.4156e+00 | dt: 16695.18 ms | lr: 1.63e-04 | tokens/s: 245.34\n",
      "Step 8/432 | loss: 7.15 | norm: 1.4023e+00 | dt: 15429.81 ms | lr: 1.86e-04 | tokens/s: 265.46\n",
      "Step 9/432 | loss: 7.09 | norm: 1.3482e+00 | dt: 16668.02 ms | lr: 2.09e-04 | tokens/s: 245.74\n",
      "Step 10/432 | loss: 7.04 | norm: 1.3221e+00 | dt: 17565.45 ms | lr: 2.33e-04 | tokens/s: 233.19\n",
      "Step 11/432 | loss: 7.00 | norm: 1.2694e+00 | dt: 17268.90 ms | lr: 2.56e-04 | tokens/s: 237.19\n",
      "Step 12/432 | loss: 6.93 | norm: 1.2410e+00 | dt: 15631.16 ms | lr: 2.79e-04 | tokens/s: 262.04\n",
      "Step 13/432 | loss: 6.86 | norm: 1.2131e+00 | dt: 15840.96 ms | lr: 3.02e-04 | tokens/s: 258.57\n",
      "Step 14/432 | loss: 6.78 | norm: 1.2090e+00 | dt: 16493.58 ms | lr: 3.26e-04 | tokens/s: 248.34\n",
      "Step 15/432 | loss: 6.67 | norm: 1.2387e+00 | dt: 17023.08 ms | lr: 3.49e-04 | tokens/s: 240.61\n",
      "Step 16/432 | loss: 6.60 | norm: 1.1582e+00 | dt: 16327.40 ms | lr: 3.72e-04 | tokens/s: 250.87\n"
     ]
    }
   ],
   "source": [
    "max_lr = 1e-3\n",
    "min_lr = max_lr * 0.1\n",
    "b1,b2,eps,weight_decay = 0.9,0.95,1e-8,0.1\n",
    "device = \"cpu\"\n",
    "optimizer = configure_adamw_optimizer(model=model, weight_decay=weight_decay, learning_rate=max_lr, betas=(b1,b2), eps=eps, device_type=device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "save_every = 1\n",
    "checkpoints_path = \"models/\"\n",
    "\n",
    "trainer_params = TrainerParams(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    train_dataloader=training_dataloader,\n",
    "    val_dataloader=validation_dataloader,\n",
    "    device=device,\n",
    "    epochs=epochs,\n",
    "    batch_size=effective_batch_size,\n",
    "    save_every=save_every,\n",
    "    checkpoints_path=checkpoints_path,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    total_steps = total_steps,\n",
    "    max_steps = max_steps,\n",
    "    warmup_steps = warmup_steps,\n",
    "    max_learning_rate=max_lr,\n",
    "    min_learning_rate=min_lr,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "trainer = Trainer(params=trainer_params)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a98c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he is turn ste ste tru vill crching toirt. Fro someone?”Lurned watching dump of the lateringching to make a smallake. But a wholeasize his to her lH more if could’t youthough, and front of gold one had un under the rest a whole hurtar So package. ourom worn B last time,” he was nuggets\\n�I a to looked him thought of themed it wornside my  pl'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = trainer.model\n",
    "prompt_tokens = torch.tensor([tokenizer.encode(text=\"he is\")]) # 1 B by 1 T\n",
    "output_tokens = model.generate(prompt_tokens, max_new_tokens=100)\n",
    "tokenizer.decode(output_tokens.tolist()[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
